---
title: Paet 3 AlexNet
createTime: 2025/08/04 16:01:57
permalink: /deeplearning/cnn/03/
---

LeNet 于 1989 年被提出，但是并未主导机器学习和计算机视觉领域。受限于硬件和数据，一直到 2012 年，卷积神经网络都被其他机器学习方法超越。

在 2012 年之前，图像特征都是机械地计算出来的——设计一套新的特征函数、改进结果、撰写论文。而与此同时一些人认为：特征本身应该被学习。于是在 2012 年，AlexNet 在 ImageNet 挑战赛上取得了极大优势。它的提出者 Alex Krizhevsky、Ilya Sutskever 和 Geoff Hinton 首次证明了学习到的特征可以超越手工设计的特征，并将这个卷积神经网络变体命名为 AlexNet。

## 1 AlexNet 结构

AlexNet 和 LeNet 非常相似，但是层数要多得多。AlexNet 由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。

![1754294922778.webp](https://oss.yoake.cc/art/deeplearning/1754294922778.webp) 

AlexNet 的第一层卷积核尺寸为 11，这是因为 ImageNet 图像比 MNIST 图像要大得多，因此需要更大的卷积窗口捕获目标。第二层卷积核尺寸为 5，然后是 3。而且 AlexNet 的通道数量比 LeNet 多得多。

在最后一层卷积层后有两个全连接层，都有 4096 个输出。

此外，AlexNet 将激活函数改为更简单的 ReLU 函数。

## 2 AlexNet 的 PyTorch 实现

AlexNet 适用于 ImageNet 的 224×224 图像，而我们所用的 MNIST 数据集为 28×28 图像。因此在加载数据之前需要将其增加到 224×224。

```py
trans = [transforms.Resize((224, 224)), transforms.ToTensor()]
```

基本代码和 LeNet 保持一致，只需要修改网络定义为：

```py
net = nn.Sequential(
    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Conv2d(96, 256, kernel_size=5, padding=2),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Conv2d(256, 384, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.Conv2d(384, 384, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.Conv2d(384, 256, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Flatten(),
    nn.Linear(6400, 4096),
    nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096, 4096),
    nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096, 10),
)
```

再将学习率改为 0.01 即可开始训练。你会注意到 AlexNet 的训练速度慢了很多，这是因为 AlexNet 更深更广，图像分辨率更高，自然训练的时间成本就更高。